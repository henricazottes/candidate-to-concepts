{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Base Stats"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Requirement already satisfied: pymupdf in c:\\users\\massa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.19.6)\n",
                  "Requirement already satisfied: unidecode in c:\\users\\massa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.3.4)\n"
               ]
            }
         ],
         "source": [
            "import sys\n",
            "!{sys.executable} -m pip install pymupdf\n",
            "!{sys.executable} -m pip install unidecode"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Load stop words\n",
            "StopWord = []\n",
            "with open('./data/stop-words/fr.txt', encoding=\"utf-8\") as file:\n",
            "    StopWords = file.read().splitlines()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [],
         "source": [
            "from os import walk, path, listdir\n",
            "import glob\n",
            "import re\n",
            "from unidecode import unidecode\n",
            "\n",
            "INPUT_PATH = path.abspath('./data/generated/input/')\n",
            "\n",
            "words_per_candidate = {}\n",
            "for type_path in ['html', 'pdf']:\n",
            "    for candidate in listdir(path.join(INPUT_PATH, type_path)):\n",
            "        if not candidate in words_per_candidate :\n",
            "            words_per_candidate[candidate] = []\n",
            "\n",
            "        text_files_path = path.join(INPUT_PATH, type_path, candidate, \"*.txt\")\n",
            "        for filepath in glob.iglob(text_files_path):\n",
            "            with open(filepath, mode='r', encoding='utf-8') as file:\n",
            "                file_text = file.read()\n",
            "\n",
            "                if type_path == 'pdf':\n",
            "                    file_text = file_text.replace(\"---PAGE---\", \"\")\n",
            "\n",
            "                words_in_document = re.sub(\"[!,’:%«»•())“/\\-\\.\\s\\d]+\", \"\\n\", file_text).lower().split()\n",
            "                \n",
            "                words_per_candidate[candidate] = words_per_candidate[candidate] + words_in_document\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Load model\n",
            "from gensim.models import Word2Vec\n",
            "filename = \"./data/model/frwiki/frwiki.gensim\"\n",
            "word2vecModel = Word2Vec.load(filename)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\massa\\AppData\\Local\\Temp\\ipykernel_19472\\545175287.py:26: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
                  "  without_stop_words = [word for word in candidate_words if word not in StopWords and word in word2vecModel ]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "macron         :  63067 ->  28215\n",
                  "pecresse       :  33768 ->  15888\n",
                  "poutou         :  11454 ->   5048\n",
                  "dupont-aignan  :  84029 ->  38635\n",
                  "hidalgo        :  14870 ->   6600\n",
                  "jadot          :  40048 ->  17855\n",
                  "lassalle       :   5216 ->   2463\n",
                  "lepen          :  76474 ->  32675\n",
                  "melenchon      : 226778 ->  98686\n",
                  "roussel        :  29165 ->  12790\n",
                  "zemmour        :  25505 ->  11516\n"
               ]
            }
         ],
         "source": [
            "from itertools import chain\n",
            "import math\n",
            "\n",
            "def calculate_tfidf(corpus):\n",
            "    number_of_documents = len(corpus)\n",
            "    \n",
            "    all_words = list(chain.from_iterable(corpus))\n",
            "    all_words_unique = set(all_words)\n",
            "\n",
            "    tf_idfs = {}\n",
            "    for current_word in all_words_unique:\n",
            "        # IDF\n",
            "        document_with_word = len([True for doc in corpus if current_word in doc])\n",
            "        idf = math.log(float(number_of_documents) / document_with_word)\n",
            "\n",
            "        # TF\n",
            "        tf = len([True for word2 in all_words if word2==current_word]) / len(all_words)\n",
            "        tf_idfs[current_word] = tf * idf\n",
            "    \n",
            "    return tf_idfs\n",
            "\n",
            "infos_per_candidate = {}\n",
            "\n",
            "for candidate, candidate_words in words_per_candidate.items(): \n",
            "    # Remove stop_words and word not in wikipedia model\n",
            "    without_stop_words = [word for word in candidate_words if word not in StopWords and word in word2vecModel ]\n",
            "\n",
            "    # Calculate frequency\n",
            "    fixed_page_word_length = 275\n",
            "    page_chunks = [without_stop_words[x:x+fixed_page_word_length] for x in range(0, len(without_stop_words), fixed_page_word_length)]\n",
            "\n",
            "    tf_idfs = calculate_tfidf(page_chunks)\n",
            "\n",
            "    word_dict = {}\n",
            "    for current_word in without_stop_words:\n",
            "        if current_word not in word_dict:\n",
            "            word_dict[current_word] = 0\n",
            "        word_dict[current_word] += 1\n",
            "\n",
            "    # Sorting\n",
            "    word_dict_items = list(word_dict.items())\n",
            "    word_dict_items.sort(key=lambda x: x[1], reverse=True)\n",
            "    word_dict = dict(word_dict_items)\n",
            "\n",
            "    # Sorting tf_idfs\n",
            "    tf_idfs_items = list(tf_idfs.items())\n",
            "    tf_idfs_items.sort(key=lambda x: x[1], reverse=True)\n",
            "    tf_idfs = dict(tf_idfs_items)\n",
            "\n",
            "    infos_per_candidate[candidate] = {\n",
            "        'nb_words': len(candidate_words),\n",
            "        'nb_without_stop': len(without_stop_words),\n",
            "        'ratio_stop': float(len(candidate_words)-len(without_stop_words))/len(candidate_words),\n",
            "        'word_map': word_dict,\n",
            "        'tf_idfs': tf_idfs\n",
            "    }\n",
            "    print(\"{:15s}: {:6d} -> {:6d}\".format(candidate, len(candidate_words), len(without_stop_words)))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Calcul de la map global des mots\n",
            "all_words_frequencies = {}\n",
            "for candidate, candidate_infos in infos_per_candidate.items():\n",
            "    nb_word_candidat = candidate_infos['nb_without_stop']\n",
            "    infos_per_candidate[candidate]['freq'] = {}\n",
            "    for word, word_count in candidate_infos['word_map'].items():\n",
            "\n",
            "        word_frequency_for_candidat = float(word_count) / nb_word_candidat\n",
            "\n",
            "        if not word in all_words_frequencies:\n",
            "            all_words_frequencies[word] = [word_frequency_for_candidat]\n",
            "        else:\n",
            "            all_words_frequencies[word].append(word_frequency_for_candidat)\n",
            "        \n",
            "        infos_per_candidate[candidate]['freq'][word] = word_frequency_for_candidat \n",
            "\n",
            "all_words_idfs = {}\n",
            "for word, frequencies in all_words_frequencies.items():\n",
            "    all_words_idfs[word] = math.log(12 / sum(frequencies))\n",
            "\n",
            "for candidate, candidate_infos in infos_per_candidate.items():\n",
            "    nb_word_candidat = candidate_infos['nb_without_stop']\n",
            "    infos_per_candidate[candidate]['tf_idfs_global'] = {}\n",
            "\n",
            "    for word in candidate_infos['word_map'].keys():\n",
            "        infos_per_candidate[candidate]['tf_idfs_global'][word] = infos_per_candidate[candidate]['freq'][word] * all_words_idfs[word]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Save infos\n",
            "import json\n",
            "import os\n",
            "OUTPUT_DIR_PATH = path.abspath('./data/generated/output/')\n",
            "output_filepath = path.join(OUTPUT_DIR_PATH, 'infos.json')\n",
            "\n",
            "if not os.path.exists(OUTPUT_DIR_PATH):\n",
            "    os.makedirs(OUTPUT_DIR_PATH)\n",
            "with open(output_filepath, 'w', encoding='utf-8') as file:\n",
            "    json.dump(infos_per_candidate, file, ensure_ascii=False, indent=2)"
         ]
      }
   ],
   "metadata": {
      "interpreter": {
         "hash": "ccbb2cd523f8445c02e4df7b4d61a10f4e94fd25c0e85e0febb2d13eee849992"
      },
      "kernelspec": {
         "display_name": "candidates-to-concepts",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.10"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
