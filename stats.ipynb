{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /home/henri/.local/share/virtualenvs/candidates-to-concepts-uvGyaLEZ/lib/python3.8/site-packages (1.19.6)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pymupdf\n",
    "!{sys.executable} -m pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load stop words\n",
    "StopWord = []\n",
    "with open('./data/stop-words/fr.txt', encoding=\"utf-8\") as file:\n",
    "    StopWords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk, path, listdir\n",
    "import glob\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "INPUT_PATH = path.abspath('./data/generated/input/')\n",
    "\n",
    "words_per_candidate = {}\n",
    "for type_path in ['html', 'pdf']:\n",
    "    for candidate in listdir(path.join(INPUT_PATH, type_path)):\n",
    "        if not candidate in words_per_candidate :\n",
    "            words_per_candidate[candidate] = []\n",
    "\n",
    "        text_files_path = path.join(INPUT_PATH, type_path, candidate, \"*.txt\")\n",
    "        for filepath in glob.iglob(text_files_path):\n",
    "            with open(filepath, mode='r', encoding='utf-8') as file:\n",
    "                file_text = file.read()\n",
    "\n",
    "                if type_path == 'pdf':\n",
    "                    file_text = file_text.replace(\"---PAGE---\", \"\")\n",
    "\n",
    "                words = re.sub(\"[!,’:%«»•())“/\\-\\.\\s\\d]+\", \"\\n\", file_text).lower().split()\n",
    "                words_per_candidate[candidate] = words_per_candidate[candidate] + words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poutou         :  11637 ->   5564\n",
      "macron         :  91226 ->  43997\n",
      "pecresse       :  36384 ->  18754\n",
      "hidalgo        :  14870 ->   7196\n",
      "lassalle       :   5216 ->   2536\n",
      "jadot          :  40048 ->  19154\n",
      "zemmour        :  25505 ->  11689\n",
      "lepen          :  76474 ->  35822\n",
      "roussel        :  29165 ->  13999\n",
      "dupont-aignan  :  84029 ->  40408\n",
      "melenchon      : 226778 -> 107495\n"
     ]
    }
   ],
   "source": [
    "infos_per_candidate = {}\n",
    "for candidate, candidate_words in words_per_candidate.items(): \n",
    "    # Remove stop_words\n",
    "    without_stop_words = [unidecode(word) for word in candidate_words if word not in StopWords ]\n",
    "    word_dict = {}\n",
    "    for current_word in without_stop_words:\n",
    "        if current_word not in word_dict:\n",
    "            word_dict[current_word] = 0\n",
    "        word_dict[current_word] += 1\n",
    "    print(\"{:15s}: {:6d} -> {:6d}\".format(candidate, len(candidate_words), len(without_stop_words)))\n",
    "    infos_per_candidate[candidate] = {\n",
    "        'nb_words': len(candidate_words),\n",
    "        'nb_without_stop': len(without_stop_words),\n",
    "        'ratio_stop': float(len(candidate_words)-len(without_stop_words))/len(candidate_words),\n",
    "        'word_map': word_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save infos\n",
    "import json\n",
    "import os\n",
    "OUTPUT_DIR_PATH = path.abspath('./data/generated/output/')\n",
    "output_filepath = path.join(OUTPUT_DIR_PATH, 'infos.json')\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR_PATH):\n",
    "    os.makedirs(OUTPUT_DIR_PATH)\n",
    "with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "    json.dump(infos_per_candidate, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbb2cd523f8445c02e4df7b4d61a10f4e94fd25c0e85e0febb2d13eee849992"
  },
  "kernelspec": {
   "display_name": "candidates-to-concepts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
